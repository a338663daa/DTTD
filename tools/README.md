# Tools

The tools in here are the main scripts used in this dataset.

Use `python tools/*****.py -h` to print a (possibly useful) help for each tool.

## calculate_camera_extrinsic.py

### Purpose

Calculate camera extrinsic. The output of this code is an extrinsic that transforms from virtual camera -> camera sensor. We call the object coordinate system that the OptiTrack tracks the "virtual camera". It is basically determined by the center of mass of the markers and assigned an arbitrary rotation. We need this extrinsic in order to apply the OptiTrack poses to the objects such that they align with the actual camera frame.

### Arguments

 1. `--scene_name`: Looks inside `EXTRINSICS_DIR` for the scene. Calculates an extrinsic using that scene. If this isn't provided, looks inside `EXTRINSICS_DIR` for the most recent extrinsic scene.

### Outputs

Places the calculated extrinsic into `cameras/******/extrinsic.txt`. The name of the camera is determined from the `scene_meta.yaml` inside the scene directory.

### Other Requirements

 1. `calculate_extrinsic/aruco_marker.txt`: The position of a marker placed in the exact center of the ARUCO marker in real life. Please use the marker that has a diameter of 1cm.

 2. `calculate_extrinsic/aruco_corners.yaml`: The position of markers placed on all the corners of the ARUCO marker in real life. Please use the smaller markers (about 5mm diameter). The naming of each corner is in accordance to the ARUCO coordinate system (quadrant 1, 2, 3, and 4).

### Class Dependencies

 1. `CameraOptiExtrinsicCalculator`: Main functionality

 2. `CameraPoseSynchronizer`: Only loads synchronized poses

## capture_data.py

### Purpose

Capture data for a scene (either data collection or extrinsic calculation). Captures RGB, depth, and meta data.

### Arguments
 1. `camera`: Required argument corresponding to the name of the camera used in this scene capture. Should match a name of a folder within `cameras/`
 2. `--scene_name`: Name of scene if creating a data collection scene, or if creating a named extrinsic scene
 3. `--extrinsic`: Creating an extrinsic calculation scene. Output will now be placed in `EXTRINSICS_DIR` instead of `SCENE_DIR`

### Outputs

Places the captured data into a `scene_dir`. If `scene_name` is defined, the name of the `scene_dir` will be that. If not, then the scene must be an extrinsic calculation scene. In this case, the name of the `scene_dir` will correspond to the current date and time. 

Outputs frames into `data_raw`, outputs scene metadata into `scene_meta.yaml`, and outputs camera necessary metadata for processing into `camera_data.csv` and `camera_time_break.csv`.

### Other Requirements

The `scene_meta.yaml` 

### Class Dependencies

 1. `AzureKinectDataCapturer`: For now, this is the only camera we support.

## create_video.py

### Purpose

Collect labels for a scene into a demo video, and place that demo video into `demos/`.

### Arguments
 1. `scene_name`: The corresponding `scene_dir` will be `SCENE_DIR/<scene_name>`

### Outputs

Places a demo video at `demos/<scene_name>.mp4` that demonstrates the semantic segmentation label generated by our manual annotation.
### Other Requirements

None

### Class Dependencies

None

## generate_dataset_root.py

### Purpose

Generates the final output for the dataset. Places outputs into a folder named `root` within the parent directory.

### Arguments
None

### Other Requirements
None

### Class Dependencies
None

## generate_scene_labeling.py

### Purpose

Uses the manual annotated poses for a single frame to generate semantic segmentation for all frames in the scene, as well as outputs metadata for each frame corresponding to the pose of the object in that frame. Optionally, can perform a refinement of the camera poses using the object poses (either via ICP or manual adjustment).

### Arguments
 1. `scene_name`: Name of the scene that will be processed.
 2. `--refine`: Perform camera pose refinement. (Defaults to True). This process takes around 5 minutes for a 200 frame scene, so not recommended until you are sure your annotations are close to correct. The refinement also relies upon the annotations in order to correct camera poses, so inaccurate poses will hurt the refinement.
 3. `--no-refine`: Don't perform camera pose refinement. Recommended option for testing/verification of annotation.
 4. `--fast`: Project fewer points onto the camera frame from each object. This will result in a much more sparse output, but runs much faster. If this flag isn't used, the runtime is between 20 minutes to an hour. If this flag is used, the runtime is less than 3 minutes. Use this flag when testing. Don't use this flag for final label output.
 5. `--use-prev-obj-pose`: Use the object poses within each frame's metadata in order to generate the labeling. This is recommended after running the algorithm once with `--refine` and `--fast`. Basically, this uses a previously calculated camera pose refinement. Setting this flag means all other flags regarding refinement are ignored (since the object poses are determined from the frame metadata and not computed).
 6. `--use-prev-refinement`: Use the previous refinement results as the synchronized poses.
 7. `--icp_refine`: Perform an ICP refinement phase if `refine` is set to True. (Defaults to True).
 8. `--no-icp-refine`: Skip the ICP refinement phase if `refine` is set to True. Recommended if using `--use-prev-refinement`, and the previous refinement already ran an ICP phase.

### Outputs

Outputs into `scene_dir/data` a label and metadata for each frame named `*****_label.png` and `*****_meta.json`. Additionally generates a debug label named `*****_label_debug.png` that has each label illustrated in brighter colors for human viewing.

If refinement is enabled, outputs a `camera_poses_synchronized_refined.csv` into `<scene_dir>/camera_poses`.

### Other Requirements

None

### Class Dependencies

 1. `CameraPoseSynchronizer`: Only loads synchronized poses
 2. `ScenePoseRefiner`: For ICP and manual camera pose refinement
 3. `MetadataGenerator`: Generates `*****_meta.json` for each frame.
 4. `SemanticLabelingGenerator`: Generates `*****_label.png` for each frame.

## manual_annotate_poses.py

### Purpose

Manual annotate the poses of the object in the scene according to the first frame of the scene. These manual pose annotations are very important to the rest of the code. We extrapolate these poses into every other frame, as well as use them to refine camera poses. 

### Arguments

 1. `scene_name`: Name of scene that will be processed. Searches inside `SCENE_DIR`.
 2. `--frame`: Which frame will be used as the manual annotation frame. Defaults to 0. Usually don't change this unless you have a good reason (the first frame doesn't show all the objects).
 3. `--use_prev`: Flag to initialize the object pose annotations using a previous set of pose annotations. (Defaults to True).
 4. `--no-use-prev`: Flag to disable initialization of object poses using a previous set of pose annotations. Not recommended.
 5. `--refresh-extrinsic`: This code archives the current camera extrinsic into `<scene_dir>/annotated_object_poses/archive_extrinsic.txt`. This is in case a new extrinsic needs to be calculated for the camera, and archiving the old extrinsic allows us to go back and annotate poses later. Use this flag in order to force the code to use the current camera extrinsic instead of the archived one. Not recommended.

### Outputs
 1. `<scene_dir>/annotated_object_poses/annotated_object_poses.yaml`: The main output of this tool. Describes the transform object -> camera for each frame in the scene.
 2. `<scene_dir>/annotated_object_poses/archive_extrinsic.txt`: The archived extrinsic (please refer to the Arguments section for an explanation.)

### Other Requirements
None

### Class Dependencies

 1. `ManualPoseAnnotator`: Main functionality
 2. `CameraPoseSynchronizer`: Just for synchronized pose loading.

## process_data.py

### Purpose

Process raw OptiTrack and raw camera data in order to

1. Synchronize the two data streams (find the corresponding OptiTrack pose for each frame from the camera)
2. Prune out bad depth images
3. Renumber frames such that the frame ID's are a contiguous sequence from 0 - n
4. Smooth OptiTrack poses using Kalman Filter

### Arguments

 1. `--scene_name`: Name of scene to process
 2. `--extrinsic`: Process an extrinsic scene instead of a data collection scene. If `scene_name` is not defined, will search `EXTRINSICS_DIR` for latest scene.
 3. `--rewrite_images`: Flag to rewrite images from `data_raw` to `data`. (Defaults to True).
 4. `--no-rewrite-images`: Flag to stop the code from rewriting renumber images from `data_raw` to `data`. Only use if you have already run this code once without this flag and are adjusting the camera poses only.

### Outputs
 1. `<scene_dir>/camera_poses/camera_poses_cleaned.csv`: Cleaned version of raw OptiTrack poses.
 2. `<scene_dir>/camera_poses/camera_poses_smoothed.csv`: Kalman Filter smoothed OptiTrack poses. Uses the cleaned poses
 3. `<scene_dir>/camera_poses_synchronized.csv`: Synchronized OptiTrack poses and frame ID's.
 4. Populates the `num_frames` field of `scene_meta.yaml`

### Other Requirements
None

### Class Dependencies

 1. `CameraPoseCleaner`: Cleans raw OptiTrack poses.
 2. `CameraPoseSynchronizer`: Synchronize camera frames with OptiTrack poses.
 3. `OptiKFSmoother`: Smooth cleaned OptiTrack poses using a Kalman Filter.

 ## process_iphone_data.py

### Purpose

Process iPhone camera data generated by `iphone_data_collection` iOS app into format outputted by `DataCapturer.py`.

Run this before running `process_data.py` for scenes captured by iPhone.

Performs undistortion on iPhone color and depth frames. Resulting frames are aligned and undistorted and depth frame is resized using nearest interpolation to color resolution.

### Arguments

 1. `scene_name`: Name of scene to process

### Outputs
 Transforms data captured by Swift iOS app inside of `iphone_data_collection` into format outputted by Azure Kinect and `data_capturing/DataCapturer.py`.

### Other Requirements
Place iPhone app output into a folder inside scene named `iphone_data`.

Organize the data like:

```
<scene name>
│   timestamps.csv (Outputted from Iphone App)
│
└───camera_poses
│   │   camera_poses.csv (OptiTrack poses)
│   
└───iphone_data
    │   0.bin
    |   0.jpeg
    |   0_calibration.txt
    |   0_distortion_table.bin
    |   ...
```

### Class Dependencies

 1. `IPhoneDataProcessor.py`: Main functionality

## sample_object_pclds.py

### Purpose

Generate `points.xyz` for each object in the `objects` folder.

### Arguments

 1. `--num_points`: How many points to sample.

### Outputs
 1. Outputs a `points.xyz` for each object folder.

### Other Requirements
None

### Class Dependencies
None

